{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4ByiP0t5ZC7E"
      },
      "source": [
        "# Approximating the Hessian for large neural networks.\n",
        "\n",
        "This notebook describes how to use the spectral-density package with Tensorflow2. The main entry point of this package is the `lanczos_algorithm.approximate_hessian` function, compatible with TensorFlow2 models.\n",
        "  - `model`: The Keras model for which we want to compute the Hessian.\n",
        "  - `dataset`: Dataset on which the model is trained. Can be a Tensorflow dataset, or more generally any iterator yielding tuples of data (X, y). If a Tensorflow dataset is used, it should be batched.\n",
        "  - `order`: Rank of the approximation of the Hessian. The higher the better the approximation. See paper for more details.\n",
        "  - `reduce_op`: Whether the loss function averages or sums the per sample loss. The default value is `MEAN` and should be compatible with most Keras losses, provided you didn't specify another reduction when instantiating the loss.\n",
        "  - `random_seed`: Seed to use to sample the first vector in the Lanczos algorithm.\n",
        "\n",
        "## Example 1: Full rank estimation for linear model.\n",
        "\n",
        "We start with a simplistic usecase: we wish to train the following model:\n",
        "\n",
        "$$ \\mbox{arg}\\max_\\beta \\sum_i (y_i - \\beta^Tx_i)^2$$\n",
        "\n",
        "As this optimization problem is quadratic, the Hessian of the loss is independent of $\\beta$ and is equal to $2X^TX$. Let's verify this using `lanczos_algorithm.approximate_hessian`, and set `order` to the number of parameters to compute the exact Hessian.\n",
        "\n",
        "We first generate some random inputs and outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "GfgWEabwJBl-"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as random\n",
        "from jax import grad, jit, vmap\n",
        "from flax import linen as nn\n",
        "import optax\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import hessian_computation\n",
        "import lanczos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "IRa00ywOdysR"
      },
      "outputs": [],
      "source": [
        "num_samples = 50\n",
        "num_features = 16\n",
        "key = random.PRNGKey(0)\n",
        "X = random.normal(key, (num_samples, num_features))\n",
        "y = random.normal(key, (num_samples,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E3MIEefYdzmW"
      },
      "source": [
        "We then define a linear model using the Flax API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "CxtMbkZSd5rk"
      },
      "outputs": [],
      "source": [
        "class LinearModel(nn.Module):\n",
        "    num_features: int\n",
        "\n",
        "    def setup(self):\n",
        "        self.dense = nn.Dense(features=1, use_bias=False)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.dense(x)\n",
        "\n",
        "# Initialize the model\n",
        "model = LinearModel(num_features=num_features)\n",
        "params = model.init(key, X)['params']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IFcGICeUeCHT"
      },
      "source": [
        "Finally, we define a loss function that takes as input the model and a batch of examples, and return a scalar loss. Here, we simply compute the mean squared error between the predictions of the model and the true outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tR4HOuQTeOBs"
      },
      "outputs": [],
      "source": [
        "def loss_fn(params, batch):\n",
        "    x, y = batch\n",
        "    preds = model.apply({'params': params}, x)\n",
        "    return jnp.mean((y - preds) ** 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-Tl3QcCYeu1D"
      },
      "source": [
        "We then define the optimizer using Optax:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xO5Cd0fQeY5c"
      },
      "outputs": [],
      "source": [
        "# Define the optimizer\n",
        "optimizer = optax.adam(learning_rate=0.001)\n",
        "opt_state = optimizer.init(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wd7t0ePTeY5c"
      },
      "source": [
        "Next, we compute the Hessian using the provided JAX implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "height": 305
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 3439,
          "status": "ok",
          "timestamp": 1584112100319,
          "user": {
            "displayName": "Pierre Foret",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgHNTtOk5LTxSObzKipclQGB_PYGFEjGL9Y65Rm=s64",
            "userId": "07616459484312176146"
          },
          "user_tz": 240
        },
        "id": "5jcoC48petZs",
        "outputId": "e4da27b3-0e5b-4927-bba2-455cf2943cfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 0/16 done in 1.85s (MVP: 1.85s).\n",
            "Iteration 1/16 done in 0.08s (MVP: 0.07s).\n",
            "Iteration 2/16 done in 0.07s (MVP: 0.07s).\n",
            "Iteration 3/16 done in 0.07s (MVP: 0.07s).\n",
            "Iteration 4/16 done in 0.07s (MVP: 0.07s).\n",
            "Iteration 5/16 done in 0.07s (MVP: 0.07s).\n",
            "Iteration 6/16 done in 0.07s (MVP: 0.07s).\n",
            "Iteration 7/16 done in 0.07s (MVP: 0.07s).\n",
            "Iteration 8/16 done in 0.07s (MVP: 0.07s).\n",
            "Iteration 9/16 done in 0.08s (MVP: 0.07s).\n",
            "Iteration 10/16 done in 0.08s (MVP: 0.07s).\n",
            "Iteration 11/16 done in 0.07s (MVP: 0.06s).\n",
            "Iteration 12/16 done in 0.07s (MVP: 0.07s).\n",
            "Iteration 13/16 done in 0.07s (MVP: 0.07s).\n",
            "Iteration 14/16 done in 0.08s (MVP: 0.07s).\n",
            "Iteration 15/16 done in 0.07s (MVP: 0.07s).\n"
          ]
        }
      ],
      "source": [
        "# Compute the Hessian using the provided JAX implementation\n",
        "hessian_matrix = hessian_computation.full_hessian(lambda p: loss_fn(p, (X, y)), params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mIZTfNzvgA__"
      },
      "source": [
        "Next, we compute the approximate Hessian matrix using the Lanczos algorithm. We need to ensure that the tangent vector `v` has the same tree structure as the parameters `params`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "height": 339
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 1215,
          "status": "ok",
          "timestamp": 1584112101605,
          "user": {
            "displayName": "Pierre Foret",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgHNTtOk5LTxSObzKipclQGB_PYGFEjGL9Y65Rm=s64",
            "userId": "07616459484312176146"
          },
          "user_tz": 240
        },
        "id": "bybJ2JKCf3aa",
        "outputId": "7f76c35d-d3ad-462e-c6f2-bc42f7a5eb76"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyEAAAFCCAYAAADmEdIuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcv\nAAAADUlEQVR42mJIS0tKjBGBgYF5AAAvZgNR0ZWl6wAAAABJRU5ErkJggg==",
            "text/plain": [
              "\u003cFigure size 1400x500 with 4 Axes\u003e"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Compute the approximate Hessian matrix using the Lanczos algorithm\n",
        "def hvp_fn(v):\n",
        "    v = jax.tree_util.tree_map(lambda x: x.flatten(), v)\n",
        "    return hessian_computation.hvp(lambda p, b: loss_fn(p, b), params, (X, y), v)\n",
        "\n",
        "order = num_features\n",
        "tridiag, vecs = lanczos.lanczos_alg(hvp_fn, num_features, order, key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hWLiHmlogwdl"
      },
      "source": [
        "We can check that the reconstructed Hessian is indeed equal to $2X^TX$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "height": 339
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 1215,
          "status": "ok",
          "timestamp": 1584112101605,
          "user": {
            "displayName": "Pierre Foret",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgHNTtOk5LTxSObzKipclQGB_PYGFEjGL9Y65Rm=s64",
            "userId": "07616459484312176146"
          },
          "user_tz": 240
        },
        "id": "bybJ2JKCf3aa",
        "outputId": "7f76c35d-d3ad-462e-c6f2-bc42f7a5eb76"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyEAAAFCCAYAAADmEdIuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcv\nAAAADUlEQVR42mJIS0tKjBGBgYF5AAAvZgNR0ZWl6wAAAABJRU5ErkJggg==",
            "text/plain": [
              "\u003cFigure size 1400x500 with 4 Axes\u003e"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Verify the reconstructed Hessian\n",
        "reconstructed_H = vecs.T @ tridiag @ vecs\n",
        "\n",
        "# Plot the Hessian matrices\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Hessian as estimated by Lanczos\")\n",
        "sns.heatmap(reconstructed_H)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"$2X^TX$\")\n",
        "sns.heatmap(2 * (X.T @ X))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cepQAqWFhRnG"
      },
      "source": [
        "## Example 2: Convnet on Cifar10\n",
        "\n",
        "We first define a VGG16-like model (15.2M parameters) that we train a bit on Cifar10:"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/deepmind/dm_python:dm_notebook3",
        "kind": "private"
      },
      "name": "Lanczos example.ipynb",
      "provenance": [
        {
          "file_id": "1fd-cGlfFirMjJ8tdbETKkmq0Nge2Ctan",
          "timestamp": 1584126624544
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}